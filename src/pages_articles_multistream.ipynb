{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import re\n",
    "import pyspark.sql\n",
    "from pyspark.sql import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import os.path\n",
    "from pyspark.sql.functions import desc\n",
    "from datetime import timedelta, date\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "#spark_hive = pyspark.sql.HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import dataframe\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import class XmlWiki\n",
    "import os\n",
    "import sys\n",
    "my_class_dir = '../src/'\n",
    "if my_class_dir not in sys.path:\n",
    "    sys.path.append(my_class_dir)\n",
    "    \n",
    "from XmlWiki import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required to iterate the range of dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nas \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading semi-structured files in Spark can be efficient if you know the **schema before accessing the data.** [link](https://szczeles.github.io/Reading-JSON-CSV-and-XML-files-efficiently-in-Apache-Spark/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/' \n",
    "WIKIPEDIA_XML_DUMP = DATA_DIR + 'pawiki-20181101-pages-articles-multistream.xml.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'XmlWiki.XmlWiki'>\n"
     ]
    }
   ],
   "source": [
    "print(XmlWiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving schema in data/pawiki-20181101-pages-articles-multistream-schema \n",
      "\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _space: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "CPU times: user 4.51 ms, sys: 2.74 ms, total: 7.25 ms\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialize object from class XmlWikidump to load the file + save schema of file\n",
    "\n",
    "WikiXML = XmlWiki(\n",
    "    path = WIKIPEDIA_XML_DUMP, \n",
    "    path_schema = DATA_DIR + 'pawiki-20181101-pages-articles-multistream-schema',\n",
    "    sampling_ratio=0.8)\n",
    "\n",
    "wikipedia = WikiXML.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+---+--------+-----+\n",
      "|_corrupt_record| id| ns|revision|title|\n",
      "+---------------+---+---+--------+-----+\n",
      "+---------------+---+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikipedia.filter(\"ns = '10'\").filter(\"title like '%infobox%'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              _VALUE|\n",
      "+--------------------+\n",
      "|{{Infobox militar...|\n",
      "|\n",
      "{{Infobox milita...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikipedia.filter(\"ns = '0'\").select(\"revision.text._VALUE\").filter(\"_VALUE like '%{{Infobox military conflict%'\").show(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38197"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pietro corrupt_id\n",
    "wikipedia.filter(\"_corrupt_record like '%<%'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65348"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.select(\"id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_corrupt_record=None),\n",
       " Row(_corrupt_record='<page>\\n    <title>HomePage</title>\\n    <ns>0</ns>\\n    <id>2</id>\\n    <redirect title=\"ਮੁੱਖ ਸਫ਼ਾ\" />'),\n",
       " Row(_corrupt_record=None),\n",
       " Row(_corrupt_record='<page>\\n    <title>ਵਿਕੀਪੀਡੀਆ:ਕੱਚਾ ਖਾਕਾ</title>\\n    <ns>4</ns>\\n    <id>785</id>\\n    <revision>\\n      <id>91046</id>\\n      <parentid>91045</parentid>\\n      <timestamp>2012-10-29T12:57:30Z</timestamp>\\n      <contributor>\\n        <username>Itar buttar</username>\\n        <id>3341</id>\\n      </contributor>\\n      <minor />'),\n",
       " Row(_corrupt_record=None),\n",
       " Row(_corrupt_record=None),\n",
       " Row(_corrupt_record=None),\n",
       " Row(_corrupt_record=None),\n",
       " Row(_corrupt_record='<page>\\n    <title>ਫਰਮਾ:ਹੋਰ ਵਿਕੀ</title>\\n    <ns>10</ns>\\n    <id>1034</id>\\n    <revision>\\n      <id>89392</id>\\n      <parentid>87208</parentid>\\n      <timestamp>2012-10-23T14:52:07Z</timestamp>\\n      <contributor>\\n        <username>Itar buttar</username>\\n        <id>3341</id>\\n      </contributor>\\n      <minor />'),\n",
       " Row(_corrupt_record='<page>\\n    <title>ਵਿਕੀਪੀਡੀਆ:Administrators</title>\\n    <ns>4</ns>\\n    <id>1035</id>\\n    <redirect title=\"ਵਿਕੀਪੀਡੀਆ:ਪ੍ਰਸ਼ਾਸਕ\" />')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.select('_corrupt_record').take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [PART 1] Get number of references per page\n",
    "\n",
    "Use the loaded public XML dump and we count the number of tag of type ```<ref>```, by parsing all pages to get the number of references.\n",
    "\n",
    "The analysis is based on work from *Research:Characterizing Wikipedia Citation Usage*: [MetaPageQueries](https://meta.wikimedia.org/wiki/Research:Characterizing_Wikipedia_Citation_Usage/First_Round_of_Analysis#Dimensions_of_Analysis)\n",
    "\n",
    "Example of reference:\n",
    "```html\n",
    "<ref>{{cite web| url=http://geonames.nga.mil/ggmagaz/geonames4.asp \n",
    "    |title=NGA GeoName Database |publisher=[[National Geospatial Intelligence Agency]] \n",
    "    |accessdate=2008-07-05 \n",
    "    |archiveurl = https://web.archive.org/web/20080608190852/http://geonames.nga.mil/ggmagaz/geonames4.asp \n",
    "    <!-- Bot retrieved archive --> |archivedate = 2008-06-08}}</ref>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namespace ns = 0 is a normal wiki page\n",
    "\n",
    "1. quantify the number of references per page\n",
    "    * using regex expression to find the references in the page\n",
    "    * counting the number of references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify the number of references per page\n",
    "\n",
    "# Compile a regular expression pattern into a regular expression object, \n",
    "#which can be used for matching using its findall.\n",
    "just_ref_regex = re.compile(r'<ref[^>]*[^\\/]>|<ref[ ]*>')\n",
    "\n",
    "# find # references <ref> per page \n",
    "def get_refs_count(entity, regex_expression=just_ref_regex ):\n",
    "    # get access to value in text in revision\n",
    "    text = entity.revision.text._VALUE\n",
    "    # find references\n",
    "    refs = just_ref_regex.findall(text)\n",
    "    print(refs)\n",
    "    return Row(id=entity.id, refs_count=len(refs))\n",
    "\n",
    "# select only pages having at least 1 revision and normal page ns = 0\n",
    "# TODO .filter(\"redirect._title is null\")\n",
    "articles = wikipedia.filter(\"ns = '0'\") \\\n",
    "                .filter(\"revision.text._VALUE is not null\") \\\n",
    "                .filter(\"length(revision.text._VALUE) > 0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the # of references per page\n",
    "# Creates a DataFrame from an RDD, apply counting function with regex expression\n",
    "reference_count_page = sqlContext.createDataFrame(articles.rdd.map(get_refs_count))\n",
    "reference_count_page.sort('refs_count', ascending=False).show(5)\n",
    "reference_count_page_sql = reference_count_page.registerTempTable('reference_count_page_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify the # articles with # ref > 0: \n",
    "non_zero_ref_query = \"\"\"\n",
    "select count(DISTINCT id)\n",
    "from reference_count_page_sql\n",
    "where refs_count > 0\n",
    "\"\"\"\n",
    "non_zero_articles = spark.sql(non_zero_ref_query)\n",
    "non_zero_articles_pd = non_zero_articles.toPandas().iloc[0,0]\n",
    "print(\"Number of articles with #references > 0: \" + str(non_zero_articles_pd))\n",
    "#non_zero_articles = reference_count_page.select('id').filter('refs_count > 0').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of pages having at least x ref counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of pages per ref count --> need to have at least x counts function\n",
    "# for x ref, how many pages have at least x ref?\n",
    "\n",
    "ref_numbers = reference_count_page.select('refs_count').distinct().sort('refs_count', ascending=True).collect()\n",
    "#xs = pd.Series(range(1,maxTagsCounts,step));\n",
    "#gratherThanData = xs.apply(lambda x: (pdTagsCounts.counts[pdTagsCounts.counts>=x]).count());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ref_count_page = reference_count_page.select('refs_count') \\\n",
    "                        .agg({'refs_count': 'max'}) \\\n",
    "                        .collect()[0]\n",
    "#max_ref_count_page[\"max(refs_count)\"]\n",
    "step = 10\n",
    "ref_numbers_pd = pd.Series(range(1,max_ref_count_page[\"max(refs_count)\"],step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_sql_view = reference_count_page.registerTempTable('references_sql_view')\n",
    "\n",
    "ref_dist = \"\"\"\n",
    "select refs_count, count('*') as Frequency \n",
    "from references_sql_view\n",
    "where refs_count > 0\n",
    "group by refs_count\n",
    "\"\"\"\n",
    "\n",
    "result_ref_dist = spark.sql(ref_dist)\n",
    "print(result_ref_dist.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of number of references** We bin the number of references to get the number of pages having at least x #references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_refs_count = reference_count_page.select('refs_count').toPandas()\n",
    "just_refs_count.plot(kind=\"hist\", bins=100, log=True, figsize=(12,7), title=\"Distribution of number of references\")\n",
    "plt.xlabel('# References')\n",
    "plt.ylabel('Frequency: # Pages having at least x references')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PART 2] Look into references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving schema in data/pawiki-20181101-pages-articles-multistream-schema \n",
      "\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _space: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "CPU times: user 4.92 ms, sys: 3.24 ms, total: 8.16 ms\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialize object from class XmlWikidump to load the file + save schema of file\n",
    "\n",
    "WikiXML_2 = XmlWiki(\n",
    "    path = WIKIPEDIA_XML_DUMP, \n",
    "    path_schema = DATA_DIR + 'pawiki-20181101-pages-articles-multistream-schema',\n",
    "    sampling_ratio=0.8)\n",
    "\n",
    "wikipedia_2 = WikiXML_2.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [('<ref name=pibmumbai>',\n",
       "   'cite press release | work=Press Information Bureau, Mumbai|publisher=Press Information Bureau, Government of India|url=http://pibmumbai.gov.in/scripts/detail.asp?releaseId=E2011IS3|title=India stats: Million plus cities in India as per Census 2011|date=31 October 2011')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get whole reference\n",
    "full_ref_regex = re.compile(r'(<ref[^>]*[^/]>|<ref[ ]*>){{([^<]*)}}</ref')\n",
    "\n",
    "def get_full_ref(entity, regex_ref=full_ref_regex):\n",
    "    text_ = entity.revision.text._VALUE\n",
    "    refs_ = regex_ref.findall(text_)\n",
    "    return refs_\n",
    "\n",
    "# filter pages to make sure you have a content for the page\n",
    "# TODO .filter(\"redirect._title is null\")\n",
    "articles_2 = wikipedia_2.filter(\"ns = '0'\") \\\n",
    "                .filter(\"revision.text._VALUE is not null\") \\\n",
    "                .filter(\"length(revision.text._VALUE) > 0\")\n",
    "references_2 = articles_2.rdd.map(get_full_ref)\n",
    "references_2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_info(entity, full_ref_regex=re.compile(r'(<ref[^>]*[^/]>|<ref[ ]*>){{([^<]*)}}</ref')):\n",
    "\n",
    "    \n",
    "    text = entity.revision.text._VALUE\n",
    "    # remove bot\n",
    "    text = re.sub(\"(<!--.*?-->)\", \"\", text, flags=re.MULTILINE)\n",
    "    refs = full_ref_regex.findall(text)\n",
    "    result = []\n",
    "    for r in refs:\n",
    "        ref_content = r[1].split(r\"|\")\n",
    "        template = ref_content.pop(0).strip()\n",
    "        properties = {}\n",
    "        for field in ref_content:\n",
    "            equal_index = field.find(\"=\")\n",
    "            field_name = field[0:equal_index].strip()\n",
    "            field_value = field[equal_index+1:].strip()\n",
    "            properties[field_name] = field_value\n",
    "        result.append(Row(id=entity.id, \n",
    "                          template=template.lower(),  \n",
    "                          url=properties.get(\"url\", \"\"), \n",
    "                          title=properties.get(\"title\")))\n",
    "    return result\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    entity:             RDD dataframe from data/**wiki-20181101-pages-articles-multistream-schema \n",
    "                        loaded with XmlWiki.py class\n",
    "    full_ref_regex:     regex expression to extract a reference\n",
    "    \n",
    "    Return:\n",
    "    result:             Spark dataframe with id, template, url, title\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top domains in the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_rrd = articles_2.rdd.flatMap(get_ref_info)\n",
    "\n",
    "references = sqlContext.createDataFrame(references_rrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------------------+--------------------+\n",
      "|  id|          template|               title|                 url|\n",
      "+----+------------------+--------------------+--------------------+\n",
      "|1050|cite press release|India stats: Mill...|http://pibmumbai....|\n",
      "|1067|          cite web|''National Anthem...|http://india.gov....|\n",
      "|1067|          cite web|Constituent Assem...|http://parliament...|\n",
      "|1067|          cite web|''National Song''...|http://india.gov....|\n",
      "|1067|          cite web|   India at a Glance|http://india.gov....|\n",
      "|1067|          cite web|India at a glance...|http://censusindi...|\n",
      "|1067|          cite web|               India|http://www.imf.or...|\n",
      "|1067|          cite web|Field Listing - D...|https://www.cia.g...|\n",
      "|1067|          cite web| Total Area of India|http://lcweb2.loc...|\n",
      "|1067|          cite web|Ethlologue report...|http://www.ethnol...|\n",
      "|1067|          cite web|           Hindustan|http://www.britan...|\n",
      "|1080|         cite book|ਗੁਰ ਸ਼ਬਦ ਰਤਨਾਕਰ ਮ...|                    |\n",
      "|1080|         cite news|ਪੰਜਾਬ ਦੇ ਪ੍ਰਸ਼ਾਸਨ...|https://www.punja...|\n",
      "|1080|         cite news|ਪੰਜਾਬ ਰਾਜ ਭਾਸ਼ਾ ਐ...|https://www.punja...|\n",
      "|1080|         cite news|ਪੰਜਾਬ ਰਾਜ ਭਾਸ਼ਾ ਐ...|https://www.punja...|\n",
      "|1080|         cite news|   ਫ਼ਾਰਸੀਆਂ ਘਰ ਗਾਲ਼ੇ|https://www.punja...|\n",
      "|1080|         cite news|ਪੰਜਾਬੀ ਤੇ ਬਿਲਾਸਪੁ...|https://www.punja...|\n",
      "|1094|         cite book|ਸਿਖਿਜ਼ਮ ਐਂਡ ਖਿ੍ਸਟ...|                    |\n",
      "|1094|         cite book|             ਸਿਖਿਜ਼ਮ|                    |\n",
      "|1094|         cite book|ਦਾ ਸੀਖਸ: ਦੇਓਰ ਰਲਿ...|                    |\n",
      "+----+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "references.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL parsing**\n",
    "\n",
    "Parse a URL into six components, returning a 6-tuple. This corresponds to the general structure of a URL: scheme://netloc/path;parameters?query#fragment. Each tuple item is a string, possibly empty.\n",
    "\n",
    "EXAMPLE: \n",
    "from urllib.parse import urlparse\n",
    "\n",
    "> o = urlparse('http://www.cwi.nl:80/%7Eguido/Python.html')\n",
    "\n",
    "> o  \n",
    "\n",
    "ParseResult(scheme='http', netloc='www.cwi.nl:80', path='/%7Eguido/Python.html',\n",
    "            params='', query='', fragment='')\n",
    "> o.scheme\n",
    "\n",
    "'http'\n",
    "\n",
    "> o.port\n",
    "\n",
    "80\n",
    "\n",
    "> o.geturl()\n",
    "\n",
    "'http://www.cwi.nl:80/%7Eguido/Python.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the top URLs. To do so, we use the `urlparse` function and apply it on each row of references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(url='http://pibmumbai.gov.in/scripts/detail.asp?releaseId=E2011IS3'),\n",
       " Row(url='http://india.gov.in/knowindia/national_anthem.php')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_domains = references.where(\"length(url)>0\")\n",
    "#£mapped_domains(row['url'])\n",
    "#not_corrupted_recs.collect()[0].not_corrupted_rec\n",
    "mapped_domains.select(\"url\").take(2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_domains = references.where(\"length(url)>0\").rdd.map(get_domain).filter(lambda row: row is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "# only get netloc which is the web address\n",
    "def get_domain(row):\n",
    "    try:\n",
    "        parsed_uri = urlparse(row['url'])\n",
    "        return ('{uri.netloc}'.format(uri=parsed_uri), 1)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "mapped_domains = references.where(\"length(url)>0\").rdd.map(get_domain).filter(lambda row: row is not None)\n",
    "domains_count = mapped_domains.reduceByKey(lambda a,b: a+b).filter(lambda r: len(r[0])>0).sortBy(lambda r: -r[1])\n",
    "domains_count.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "domains_count.map(lambda r: r[1]).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_row(rdd):\n",
    "    return Row(domain=rdd[0], count=rdd[1], perc=rdd[1]/18908198)\n",
    "\n",
    "domains_distribution = sqlContext.createDataFrame(domains_count.map(to_row)).sort(\"count\", ascending=False)\n",
    "domains_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = domains_distribution.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head(15).plot(kind='bar',\n",
    "                  x='domain',\n",
    "                  y='perc',\n",
    "                  figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the pageviews by page\n",
    "\n",
    "In this preliminary analysis, we use the table 'webrequest' to get the number of times a page is loaded and to evaluate the click rate. In the next data collection, we will have a 'page-load' event.\n",
    "\n",
    "Get the pageviews by day (step 1) and aggregate (step 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User must be NOT loggedIn, not a bot and the view must be in the English version of Wikipedia\n",
    "# The views are aggregated by page, country, and access method\n",
    "\n",
    "start_date = date(2018, 6, 29)\n",
    "end_date = date(2018, 7, 9)\n",
    "\n",
    "# step 1\n",
    "links_query = \"\"\"\n",
    "select page_id, geocoded_data.continent, geocoded_data.country_code, access_method, count(*) as pageviews\n",
    "from wmf.webrequest \n",
    "where day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND x_analytics_map['loggedIn'] is NULL\n",
    "AND namespace_id = 0\n",
    "AND agent_type = 'user'\n",
    "AND is_pageview = TRUE\n",
    "AND (uri_host = 'en.wikipedia.org' OR uri_host = 'en.m.wikipedia.org')\n",
    "AND access_method <> 'mobile app'\n",
    "group by page_id, geocoded_data.continent, geocoded_data.country_code, access_method\n",
    "\"\"\"\n",
    "\n",
    "pageviews = sc.emptyRDD()\n",
    "for d in daterange(start_date, end_date):\n",
    "    views_info = spark.sql(links_query.format(d.day, d.month, d.year))\n",
    "    pageviews = pageviews.union(views_info.rdd)\n",
    "\n",
    "\n",
    "# Convert the access_method string to be consistent with the events table\n",
    "def convert_name(row):\n",
    "    mode = 'desktop'\n",
    "    if row['access_method'].startswith('mobile'):\n",
    "        mode = 'mobile'\n",
    "    return Row(page_id=row['page_id'], \n",
    "               continent=row['continent'], \n",
    "               country_code=row['country_code'], \n",
    "               access_method=mode,\n",
    "               pageviews=row['pageviews']\n",
    "              )\n",
    "\n",
    "\n",
    "views_info_daily = sqlContext.createDataFrame(pageviews.map(convert_name).filter(lambda row: row is not None))\n",
    "views_info_daily.registerTempTable(\"views_info_daily\")\n",
    "\n",
    "# step 2\n",
    "aggregate_query = \"\"\"\n",
    "select page_id, continent, country_code, access_method, sum(pageviews) as pageviews\n",
    "from views_info_daily\n",
    "group by page_id, continent, country_code, access_method\n",
    "\"\"\"\n",
    "\n",
    "views_info = spark.sql(aggregate_query).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the number of visit with at least 1 extClick events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2018, 6, 29)\n",
    "end_date = date(2018, 7, 9)\n",
    "\n",
    "# step 1\n",
    "events_query = \"\"\"\n",
    "select event.page_id, geocoded_data.continent, geocoded_data.country_code, \n",
    "        event.mode, count(distinct(event.session_token)) total \n",
    "from event.citationusage\n",
    "where wiki = 'enwiki'\n",
    "AND day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND useragent.is_bot = FALSE\n",
    "and event.in_infobox = FALSE\n",
    "and event.footnote_number IS NOT NULL\n",
    "and event.action = 'extClick'\n",
    "group by event.page_id, geocoded_data.continent, geocoded_data.country_code, event.mode\n",
    "\"\"\"\n",
    "\n",
    "events_rdd = sc.emptyRDD()\n",
    "for d in daterange(start_date, end_date):\n",
    "    daily_events = spark.sql(events_query.format(d.day, d.month, d.year))\n",
    "    events_rdd = events_rdd.union(daily_events.rdd)\n",
    "\n",
    "events_merged = sqlContext.createDataFrame(events_rdd)\n",
    "events_merged.registerTempTable(\"events_merged\")\n",
    "\n",
    "# step 2\n",
    "aggregate_query = \"\"\"\n",
    "select page_id, continent, country_code, mode, sum(total) extClick_count\n",
    "from events_merged\n",
    "group by page_id, continent, country_code, mode\n",
    "\"\"\"\n",
    "\n",
    "clicks_count = spark.sql(aggregate_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the click rate\n",
    "\n",
    "Step 1: Filter out all the articles that do NOT contain any reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_info.registerTempTable('views_info')\n",
    "references_count.registerTempTable('references_count')\n",
    "\n",
    "# join to filter out the pages without references\n",
    "pages_with_refs_sql = \"\"\"\n",
    "select vi.*, rc.refs_count\n",
    "from views_info vi\n",
    "join references_count rc\n",
    "on vi.page_id = rc.id\n",
    "where rc.refs_count > 0\n",
    "\"\"\"\n",
    "\n",
    "pages_with_refs = spark.sql(pages_with_refs_sql)\n",
    "pages_with_refs.cache().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: join with the external clicks count. Left join, set 0 clicks when there are no events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_count.registerTempTable('clicks_count')\n",
    "pages_with_refs.registerTempTable('pages_with_refs')\n",
    "\n",
    "\n",
    "pages_with_refs_sql = \"\"\"\n",
    "select pr.*,\n",
    "    CASE WHEN ce.extClick_count is NULL THEN 0 ELSE extClick_count END AS extClick_count\n",
    "from pages_with_refs pr\n",
    "left join clicks_count ce\n",
    "on pr.page_id = ce.page_id\n",
    "and pr.continent = ce.continent\n",
    "and pr.country_code = ce.country_code\n",
    "and pr.access_method = ce.mode\n",
    "\"\"\"\n",
    "\n",
    "all_info = spark.sql(pages_with_refs_sql)\n",
    "\n",
    "all_info.registerTempTable('all_info')\n",
    "with_ratio_sql = \"\"\"\n",
    "select *, extClick_count / pageviews as clickrate\n",
    "from all_info\n",
    "\"\"\"\n",
    "\n",
    "all_info = spark.sql(with_ratio_sql)\n",
    "all_info.cache().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pages with events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info.where('extClick_count>0').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get click rate by country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info.registerTempTable('all_info')\n",
    "\n",
    "avg_by_country = \"\"\"\n",
    "select country_code, access_method, avg(clickrate) clickrate_avg, count(*) count, sum(pageviews) total_views\n",
    "from all_info\n",
    "group by country_code, access_method\n",
    "sort by clickrate_avg desc\n",
    "\"\"\"\n",
    "\n",
    "clickrate_by_country = spark.sql(avg_by_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = clickrate_by_country.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove countries where there are less that 100 views in 1 week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = cr[cr.total_views>100].sort_values(by='clickrate_avg', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Top 15 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr[cr.access_method=='desktop'].head(15).plot(kind='bar', \n",
    "                                              x='country_code', \n",
    "                                              y='clickrate_avg',\n",
    "                                              figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Last 15 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr[cr.access_method=='desktop'].tail(15).plot(kind='bar', \n",
    "                                              x='country_code', \n",
    "                                              y='clickrate_avg',\n",
    "                                              figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains analysis - extClick events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_domain(row):\n",
    "    try:\n",
    "        parsed_uri = urlparse(row['link_url'])\n",
    "        return ('{uri.netloc}'.format(uri=parsed_uri), 1)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "start_date = date(2018, 6, 29)\n",
    "end_date = date(2018, 7, 9)\n",
    "\n",
    "# step 1\n",
    "events_query = \"\"\"\n",
    "select event.link_url, 1 as count\n",
    "from event.citationusage\n",
    "where wiki = 'enwiki'\n",
    "AND day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND useragent.is_bot = FALSE\n",
    "and event.in_infobox = FALSE\n",
    "and event.footnote_number IS NOT NULL\n",
    "and event.action = 'extClick'\n",
    "\"\"\"\n",
    "\n",
    "links_rdd = sc.emptyRDD()\n",
    "for d in daterange(start_date, end_date):\n",
    "    daily_events = spark.sql(events_query.format(d.day, d.month, d.year))\n",
    "    links_rdd = links_rdd.union(daily_events.rdd)\n",
    "\n",
    "\n",
    "links_rdd = links_rdd.map(get_domain)\\\n",
    "            .filter(lambda row: row is not None)\\\n",
    "            .reduceByKey(lambda a,b: a+b)\\\n",
    "            .filter(lambda r: len(r[0])>0).map(lambda r: Row(domain=r[0], count=r[1]))\n",
    "    \n",
    "links_merged = sqlContext.createDataFrame(links_rdd).sort(\"count\", ascending=False)\n",
    "links_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_urls = links_merged.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Top 15 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_urls.head(15).plot(kind='bar', \n",
    "                  x='domain', \n",
    "                  y='count',\n",
    "                  figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

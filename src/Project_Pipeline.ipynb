{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wptools\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import *\n",
    "import os.path\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import dataframe\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import get_ref_info.py\n",
    "import os\n",
    "import sys\n",
    "my_fun = '../src/'\n",
    "if my_fun not in sys.path:\n",
    "    sys.path.append(my_fun)\n",
    "    \n",
    "from get_ref_info import *\n",
    "from operator import add\n",
    "from urllib.parse import urlparse\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File schema** is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- restrictions: string (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _deleted: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _deleted: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- minor: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _xml:space: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- good_categories: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../' \n",
    "WIKIPEDIA_CONFLICTS_PARQUET = DATA_DIR + 'selectedAllConflict.parquet'\n",
    "\n",
    "# loading the saved parquet files\n",
    "wikipedia_ = spark.read.parquet(WIKIPEDIA_CONFLICTS_PARQUET)\n",
    "# show file schema\n",
    "wikipedia_.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 18782\n"
     ]
    }
   ],
   "source": [
    "# how many articles are there?\n",
    "print(\"Number of articles: {}\".format(wikipedia_.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Article outliers removal\n",
    "\n",
    "We have some outliers we need to deal with, as a first step we use keywords (**\"war, riot, conflict, protest, revolt, operation, attack, annexation, genocide, insurgency, crisis, confrontation, clash\"**) on the article titles to extract from our chosen categories (`civilian attack`, `civil conflict`, `military conflict`).\n",
    "\n",
    "For example, we found that some articles in `civilian attack` may contain the infobox `civilian attack` but are not really a `civilian attack`. The following page \n",
    "\n",
    "> Fraunces Tavern: Fraunces Tavern is a landmark museum and restaurant in New York City, situated at 54 Pearl Street at the corner of Broad Street. The location played a prominent role in history before, during and after the American Revolution, serving as a headquarters for George Washington, a venue for peace negotiations with the British, and housing federal offices in the Early Republic.\n",
    "\n",
    "Since some pages are considered **outliers**, we remove them using the method described above and remove any disambiguations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles without outliers: 1788\n"
     ]
    }
   ],
   "source": [
    "# filter out outliers\n",
    "outlier_regex = \"war*|riot*|conflict*|protest*|revolt*|operation*|attack*|annexation*|genocide*|insurgency*|crisis*|confrontation*|clash*\"\n",
    "wikipedia = wikipedia_.filter(wikipedia_['title'].rlike(outlier_regex))\n",
    "print(\"Number of articles without outliers: {}\".format(wikipedia.count()))\n",
    "\n",
    "wikipedia = wikipedia.filter(\"lower(revision.text._VALUE) not like '%{disambiguation}%'\")\n",
    "# save in parquet\n",
    "wikipedia.write.parquet(DATA_DIR+\"wikipedia_no_outliers.parquet\")\n",
    "# load from parquet so that the rest of notebook uses cleaned version of data\n",
    "wikipedia = spark.read.parquet(DATA_DIR+\"wikipedia_no_outliers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisql = wikipedia.registerTempTable('wikisql') \n",
    "\n",
    "query = \"\"\"\n",
    "select title\n",
    "from wikisql\n",
    "where categories = 'military conflict'\n",
    "\"\"\"\n",
    "\n",
    "result_c = spark.sql(query)\n",
    "result_c.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we explore how many articles have each keyword selected in the given keywords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quantifying *importance* of each page in each category\n",
    "\n",
    "We want to see how *important* each page is in each category. As we are solely focusing on *'war'*-related subjects in this pilot phase, we define *page importance* by the number of deaths. \n",
    "\n",
    "## Infobox or Wikidata per category \n",
    "\n",
    "Functions `get_wiki_civilian_attack`, `get_wiki_military_conflict`\n",
    "to get relevant information and views for each category: `civilian attack`, `military conflict`. \n",
    "Data is obtained either using the page's wikidata is the data exists or acquired through infobox parsing.\n",
    "Relevant information is chosen based on the fields found on [List of infoboxes and fields](https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes#Event) \n",
    "\n",
    "Extract info for each category in **Infobox**:\n",
    "* `civilian attack`\n",
    "    * location\n",
    "    * date \n",
    "    * fatalities\n",
    "* `military conflict`\n",
    "    * place\n",
    "    * date \n",
    "    * casualities1\n",
    "* `civil conflict`\n",
    "    * place\n",
    "    * date \n",
    "    * casualities1\n",
    "    \n",
    "Extract info for each category in **Wikidata**:\n",
    "* `civilian attack`\n",
    "    * location\n",
    "    * date \n",
    "    * fatalities\n",
    "* `military conflict`\n",
    "    * 'number of deaths (P1120)']['amount']\n",
    "    * 'end time (P582)'\n",
    "    * 'location (P276)'\n",
    "* `civil conflict`\n",
    "    * 'number of deaths (P1120)']['amount']\n",
    "    * 'end time (P582)'\n",
    "    * 'location (P276)'\n",
    "    \n",
    "We use an external library [wptools wiki](https://github.com/siznax/wptools/wiki) to help us parse the data. \n",
    "\n",
    "### Categories\n",
    "We focus on categories : `civilian attack`, `military conflict`, `civil conflict` from our pre-filtered wikipedia dump data which had 3 categories: `civilian attack`, `military conflict`, `civil conflict` (key words we used via regex expression to extract articles containing infoboxes with such headers by parsing the revision.text.__VALUE). \n",
    "\n",
    "We also found a list of all ongoing civil wars listed on [wikipedia](https://en.wikipedia.org/wiki/List_of_civil_wars#Ongoing_civil_wars) have a `Template:Infobox military`. This may be of interest in a later step.\n",
    "\n",
    "[List of ongoing civil wars](https://en.wikipedia.org/wiki/List_of_civil_wars#Ongoing_civil_wars):\n",
    " * Myanmar, Internal conflict in Myanmar, since 1948\n",
    " * Indonesia, Papua conflict, since 1962\n",
    " * Colombia, Colombian conflict, since 1964\n",
    " * Afghanistan, War in Afghanistan, since 1978\n",
    " * Turkey, Kurdish–Turkish conflict since 1978\n",
    " * Somalia, Somali Civil War, since 1988\n",
    " * Sudan, three conflicts\n",
    "* War in Darfur, since 26 February 2003\n",
    "* Sudanese nomadic conflicts, since 26 May 2009 through at least 2017\n",
    "* Sudanese conflict in South Kordofan and Blue Nile since 5 June 2011\n",
    " * Pakistan, War in North-West Pakistan, since 16 March 2004\n",
    " * Paraguay, Paraguayan People's Army insurgency, since 2005\n",
    " * Syria, Syrian Civil War, since 15 March 2011, also see List of armed groups in the Syrian Civil War\n",
    " * Central African Republic, Central African Republic conflict, since 10 December 2012\n",
    " * South Sudan, South Sudanese Civil War, since 15 December 2013\n",
    " * Libya, Second Libyan Civil War, since 16 May 2014\n",
    " * Yemen, Second Yemeni Civil War, since 19 March 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_military_conflict(entity):\n",
    "    page = wptools.page(entity.title)\n",
    "    # extract relevant information and put in dictionary\n",
    "    info = {'death': None, 'end_date': None, 'location': None}\n",
    "    \n",
    "    try: \n",
    "        page.get_wikidata()\n",
    "        info['death'] = page.data['wikidata']['number of deaths (P1120)']['amount']\n",
    "    \n",
    "    except:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['death'] = poly_page.data['infobox']['casualties1']\n",
    "        except:\n",
    "            info['death'] = None\n",
    "            \n",
    "    try:\n",
    "        page.get_wikidata()\n",
    "        info['end_date'] = page.data['wikidata']['end time (P582)']\n",
    "    except:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['end_date'] = poly_page.data['infobox']['date']\n",
    "        except:\n",
    "            info['end_date'] = None\n",
    "    \n",
    "    try:\n",
    "        page.get_wikidata()\n",
    "        info['location'] = page.data['wikidata']['location (P276)']\n",
    "    except:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['location'] = poly_page.data['infobox']['place']\n",
    "        except:\n",
    "            info['location'] = None\n",
    "    \n",
    "    \n",
    "    return Row(id=entity.id, title=entity.title, death=info['death'],\n",
    "               end_date=info['end_date'], location=info['location'])\n",
    "\n",
    "def get_wiki_civilian_attack(entity):\n",
    "    page = wptools.page(entity.title)\n",
    "    # extract relevant information and put in dictionary\n",
    "    info = {'death': None, 'end_date': None, 'location': None}\n",
    "    \n",
    "    try: \n",
    "        page.get_wikidata()\n",
    "        info['death'] = page.data['wikidata']['number of deaths (P1120)']['amount']\n",
    "    \n",
    "    except:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['death'] = poly_page.data['infobox']['fatalities']\n",
    "        except:\n",
    "            info['death'] = None\n",
    "            \n",
    "    try:\n",
    "        page.get_wikidata()\n",
    "        info['end_date'] = page.data['wikidata']['end time (P582)']\n",
    "    except:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['end_date'] = poly_page.data['infobox']['date']\n",
    "        except:\n",
    "            info['end_date'] = None\n",
    "    \n",
    "    try:\n",
    "        page.get_wikidata()\n",
    "        info['location'] = page.data['wikidata']['location (P276)']\n",
    "    except:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['location'] = poly_page.data['infobox']['location']\n",
    "        except:\n",
    "            info['location'] = None\n",
    "    \n",
    "    \n",
    "    return Row(id=entity.id, title=entity.title, death=info['death'],\n",
    "               end_date=info['end_date'], location=info['location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata & Infobox `military conflict `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_military_conflict = 'military conflict'\n",
    "# find all pages that have category military conflict\n",
    "wiki_military_conflict = wikipedia.where(\"categories like '%{}%'\".format(infobox_military_conflict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_military_conflict_df = sqlContext.createDataFrame(wiki_military_conflict.rdd.map(get_wiki_military_conflict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_military_conflict_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving binary file to future uses\n",
    "#wiki_military_conflict_df.write.parquet(DATA_DIR +\"{}.parquet\".format(infobox_military_conflict));\n",
    "# loading the saved parquet files\n",
    "#wiki_military_conflict_df_reload = spark.read.parquet(\n",
    "    #DATA_DIR_FILTERED+\"{}.parquet\".format(infobox_military_conflict));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata & Infobox `civilian attack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_civilian_attack = 'civilian attack'\n",
    "# find all pages that have category civilian attack\n",
    "wiki_civilian_attack = wikipedia.where(\"categories like '%{}%'\".format(infobox_civilian_attack)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civilian_attack_df = sqlContext.createDataFrame(wiki_civilian_attack.rdd.map(get_wiki_civilian_attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiki_civilian_attack_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata & Infobox `civil conflict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_civil_conflict = 'civil conflict'\n",
    "# find all pages that have category civilian attack\n",
    "wiki_civil_conflict = wikipedia.where(\"categories like '%{}%'\".format(infobox_civil_conflict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_conflict_df = sqlContext.createDataFrame(wiki_civil_conflict.rdd.map(get_wiki_military_conflict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_conflict_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Popularity Metrics\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "DATA_DIR = '../' \n",
    "WIKIPEDIA_REFERENCES_PARQUET = DATA_DIR + \"wikipedia_no_outliers.parquet\"\n",
    "\n",
    "# loading the saved parquet files\n",
    "wikipedia_ref = spark.read.parquet(WIKIPEDIA_REFERENCES_PARQUET)\n",
    "wikipedia_ref.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POPULARITY METRIC 1\n",
    "\n",
    "### Get number of references per page\n",
    "\n",
    "We use our pre-filtered data and we count the number of tag of type ```<ref>```, by parsing all pages to get the number of references. We think that the more *popular* a page is, the more references it will contain.\n",
    "\n",
    "We want to see:\n",
    "* how many references a page has\n",
    "* what does the distribution of the references look like \n",
    "* what are the top domains across these articles \n",
    "* how many articles have a reference to each domain\n",
    "\n",
    "The analysis is based on work from *Research:Characterizing Wikipedia Citation Usage*: [MetaPageQueries](https://meta.wikimedia.org/wiki/Research:Characterizing_Wikipedia_Citation_Usage/First_Round_of_Analysis#Dimensions_of_Analysis)\n",
    "\n",
    "Example of reference format:\n",
    "```html\n",
    "<ref>{{cite web| url=http://geonames.nga.mil/ggmagaz/geonames4.asp \n",
    "    |title=NGA GeoName Database |publisher=[[National Geospatial Intelligence Agency]] \n",
    "    |accessdate=2008-07-05 \n",
    "    |archiveurl = https://web.archive.org/web/20080608190852/http://geonames.nga.mil/ggmagaz/geonames4.asp \n",
    "    <!-- Bot retrieved archive --> |archivedate = 2008-06-08}}</ref>\n",
    "```\n",
    "\n",
    "**quantify the number of references per page**\n",
    "    * using regex expression to find the references in the page\n",
    "    * counting the number of references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify the number of references per page\n",
    "\n",
    "# Compile a regular expression pattern into a regular expression object, \n",
    "#which can be used for matching using its findall.\n",
    "just_ref_regex = re.compile(r'<ref[^>]*[^\\/]>|<ref[ ]*>')\n",
    "\n",
    "# find # references <ref> per page \n",
    "def get_refs_count(entity, regex_expression=just_ref_regex ):\n",
    "    # get access to value in text in revision\n",
    "    text = entity.revision.text._VALUE\n",
    "    # find references\n",
    "    refs = just_ref_regex.findall(text)\n",
    "    return Row(id=entity.id, refs_count=len(refs), categories=entity.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the # of references per page\n",
    "# Creates a DataFrame from an RDD, apply counting function with regex expression\n",
    "reference_count_page = sqlContext.createDataFrame(wikipedia_ref.rdd.map(get_refs_count))\n",
    "reference_count_page.sort('refs_count', ascending=False).show(5)\n",
    "reference_count_page_sql = reference_count_page.registerTempTable('reference_count_page_sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify the # articles with # ref > 0: \n",
    "non_zero_ref_query = \"\"\"\n",
    "select count(DISTINCT id)\n",
    "from reference_count_page_sql\n",
    "where refs_count > 0\n",
    "\"\"\"\n",
    "non_zero_articles = spark.sql(non_zero_ref_query)\n",
    "\n",
    "non_zero_articles_pd = non_zero_articles.toPandas().iloc[0,0]\n",
    "print(\"Number of articles with #references > 0: \" + str(non_zero_articles_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of number of references** We bin the number of references to get the number of pages having at least x #references. This is observed with a histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_refs_count = reference_count_page.select('refs_count').toPandas()\n",
    "just_refs_count.plot(kind=\"hist\", bins=100, log=True, figsize=(12,7), title=\"Distribution of number of references\")\n",
    "plt.xlabel('# References')\n",
    "plt.ylabel('Frequency: # Pages having at least x references')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of number of references**: It seems that the distribution may follow a power law. To verify our suspicions, we plot our cumulative distribution in normal axes and in log-log axes. We look at a \"cumulative distribution\": how many references have at least x pages?\n",
    "\n",
    "The distribution has a heavy-tailed distribution: they may follow a power law. To recognize if the distirbutions follow a power law, we plot the data in log-log axes. If they follow a power law, reporting mean or variance for a power-law-distributed data is not relevant as these statistics are not robust as they are sensitive to data. Instead, we use robust statistics and report the median and quartiles using boxplots. We compare boxplots with and without outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the domains of references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_rrd = wikipedia.rdd.flatMap(get_ref_info)\n",
    "references = sqlContext.createDataFrame(references_rrd)\n",
    "references.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL parsing** \n",
    "Parse a URL into six components, returning a 6-tuple. This corresponds to the general structure of a URL: scheme://netloc/path;parameters?query#fragment. Each tuple item is a string, possibly empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(row):\n",
    "    # parse url and return for each row (url, 1) where 1 is the occurence of ref = 1 for that page\n",
    "    try:\n",
    "        parsed_uri = urlparse(row['url'])\n",
    "        return ('{uri.netloc}'.format(uri=parsed_uri), 1)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "mapped_domains = references.where(\"length(url)>0\").rdd.map(get_domain).filter(lambda row: row is not None)\n",
    "# for each domain count how many references there are in total to find top domains\n",
    "domains_count = mapped_domains.reduceByKey(lambda a,b: a+b).filter(lambda r: len(r[0])>0).sortBy(lambda r: -r[1])\n",
    "domains_count.take(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of links\n",
    "number_links = domains_count.map(lambda r: r[1]).reduce(add)\n",
    "print(\"Total number of links: {}\".format(number_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(rdd):\n",
    "    return Row(domain=rdd[0], count=rdd[1], perc=rdd[1]*100/number_links)\n",
    "\n",
    "domains_distribution = sqlContext.createDataFrame(domains_count.map(percentage)).sort(\"count\", ascending=False)\n",
    "domains_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_distribution_pd = domains_distribution.toPandas().set_index('domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_distribution_pd20 = domains_distribution_pd.head(15)\n",
    "import seaborn as sns\n",
    "# plot using bar plot\n",
    "f, ax1 = plt.subplots()\n",
    "plt.sca(ax1)\n",
    "sns.barplot(domains_distribution_pd20['perc'], domains_distribution_pd20.index, palette=\"RdBu\", ax=ax1, orient='h')\n",
    "plt.xlabel('Percentage of references [%]',fontsize=18)\n",
    "plt.ylabel('Domains',fontsize=18)\n",
    "f.suptitle('Bar plot of percentage of references for domains', fontsize=20)\n",
    "f.set_size_inches(10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity metric 2\n",
    "\n",
    "### Get number of views per page\n",
    "\n",
    "A way of measuring the popularity of a page can be done by looking at the number of views per page. Let's follow the trend!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity metric 3\n",
    "\n",
    "### Get number of external links per page\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity metric 3\n",
    "\n",
    "### Get length of article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "DATA_DIR = '../' \n",
    "WIKIPEDIA_LENGTH_PARQUET = DATA_DIR + \"wikipedia_no_outliers.parquet\"\n",
    "\n",
    "# loading the saved parquet files\n",
    "articles = spark.read.parquet(WIKIPEDIA_LENGTH_PARQUET)\n",
    "articles_length = articles.withColumn('article_lenght', F.length(articles.revision.text._VALUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== TO DELETE ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_civil_attack = 'civilian attack'\n",
    "# find all pages that have category civilian attack\n",
    "wiki_civil_attack = wikipedia.where(\"categories like '%{}%'\".format(infobox_civil_attack)) \n",
    "# show file schema\n",
    "wiki_civil_attack.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_attack.filter(\"title like '%Fraunces Tavern%'\").select(\"categories\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_attack_df = sqlContext.createDataFrame(wiki_civil_attack.rdd.map(get_infobox_civilian_attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_attack_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET INFO FROM INFOBOX + VIEWS for different categories\n",
    "def get_infobox_civilian_attack(entity):\n",
    "    # get page\n",
    "    page = wptools.page(entity.title)\n",
    "    page.get_parse()\n",
    "    page.get_more()\n",
    "    # extract relevant information and put in dictionary\n",
    "    info = {'views': None, 'location': None, \n",
    "            'date': None, 'fatalities': None, 'injuries': None }\n",
    "    try:\n",
    "        info['views'] = page.data['views']\n",
    "    except KeyError:\n",
    "        info['views'] = None\n",
    "    \n",
    "    for ele in list(info.keys())[1:]:\n",
    "        try:\n",
    "            info[ele] = page.data['infobox'][ele]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return Row(id=entity.id, title=entity.title, location=info['location'], views=info['views'], date=info['date'], \n",
    "                fatalities=info['fatalities'], injuries=info['injuries'])\n",
    "\n",
    "def get_infobox_civil_conflict(entity):\n",
    "    # get page\n",
    "    page = wptools.page(entity.title)\n",
    "    page.get_parse()\n",
    "    page.get_more()\n",
    "    # extract relevant information and put in dictionary\n",
    "    info = {'views': None, 'place': None, 'injuries': None,\n",
    "            'date': None, 'fatalities': None, 'casualties1': None, 'casualties2': None,\n",
    "            'leadfigures1': None, 'leadfigures2': None} \n",
    "    try:\n",
    "        info['views'] = page.data['views']\n",
    "    except KeyError:\n",
    "        info['views'] = None\n",
    "    \n",
    "    for ele in list(info.keys())[1:]:\n",
    "        try:\n",
    "            info[ele] = page.data['infobox'][ele]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return Row(id=entity.id, title=entity.title, location=info['place'], views=info['views'], date=info['date'], \n",
    "               fatalities=info['fatalities'], casualties1=info['casualties1'], casualties2=info['casualties2'],\n",
    "               injuries=info['injuries'], leadfigures1=info['leadfigures1'], leadfigures2=info['leadfigures2'])\n",
    "\n",
    "def get_infobox_military_conflict(entity):\n",
    "    # get page\n",
    "    page = wptools.page(entity.title)\n",
    "    page.get_parse()\n",
    "    page.get_more()\n",
    "    # extract relevant information and put in dictionary\n",
    "    info = {'views': None, 'place': None, \n",
    "            'date': None, 'casualties1': None, 'casualties2': None}#, 'status': None}\n",
    "            #'combatant1': None, 'combatant2': None, 'status': None} \n",
    "    try:\n",
    "        info['views'] = page.data['views']\n",
    "    except KeyError:\n",
    "        info['views'] = None\n",
    "    \n",
    "    for ele in list(info.keys())[1:]:\n",
    "        try:\n",
    "            info[ele] = page.data['infobox'][ele]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return Row(id=entity.id, title=entity.title, location=info['place'], views=info['views'], date=info['date'], \n",
    "               casualties1=info['casualties1'], casualties2=info['casualties2']) #, status=info['status'])\n",
    "               #combatant1=info['combatant1'], combatant2=info['combatant2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_FILTERED = '../clean_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving binary file to future uses\n",
    "wiki_civil_attack_df.write.parquet(DATA_DIR_FILTERED+\"{}.parquet\".format(infobox_civil_attack));\n",
    "# loading the saved parquet files\n",
    "wiki_civil_attack_df_reload = spark.read.parquet(DATA_DIR_FILTERED+\"{}.parquet\".format(infobox_civil_attack));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_attack_df = sqlContext.createDataFrame(wiki_civil_attack_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infobox `civil conflict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_civil_conflict = 'civil conflict'\n",
    "# find all pages that have category civil conflict\n",
    "wiki_civil_conflict = wikipedia.where(\"categories like '%{}%'\".format(infobox_civil_conflict)) \n",
    "# show file schema\n",
    "wiki_civil_conflict.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_conflict_df = sqlContext.createDataFrame(wiki_civil_conflict.rdd.map(get_infobox_civil_conflict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_civil_conflict_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving binary file to future uses\n",
    "wiki_civil_conflict_df.write.parquet(DATA_DIR_FILTERED+\"{}.parquet\".format(infobox_civil_conflict));\n",
    "# loading the saved parquet files\n",
    "wiki_civil_conflict_df_reload = spark.read.parquet(DATA_DIR_FILTERED+\"{}.parquet\".format(infobox_civil_conflict));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infobox `military conflict `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_military_conflict = 'military conflict'\n",
    "# find all pages that have category military conflict\n",
    "wiki_military_conflict = wikipedia.where(\"categories like '%{}%'\".format(infobox_military_conflict)) \n",
    "# show file schema\n",
    "wiki_military_conflict.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_military_conflict_df = sqlContext.createDataFrame(wiki_military_conflict.rdd.map(get_infobox_military_conflict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_military_conflict_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving binary file to future uses\n",
    "wiki_military_conflict_df.write.parquet(DATA_DIR_FILTERED+\"{}.parquet\".format(infobox_military_conflict));\n",
    "# loading the saved parquet files\n",
    "wiki_military_conflict_df_reload = spark.read.parquet(\n",
    "    DATA_DIR_FILTERED+\"{}.parquet\".format(infobox_military_conflict));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_military_conflict(entity):\n",
    "    page = wptools.page(entity.title)\n",
    "    # extract relevant information and put in dictionary\n",
    "    info = {'death': None, 'end_date': None, 'location': None}\n",
    "    \n",
    "    try: \n",
    "        page.get_wikidata()\n",
    "        info['death'] = page.data['wikidata']['number of deaths (P1120)']['amount']\n",
    "    \n",
    "    except KeyError:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['death'] = poly_page.data['infobox']['casualties1']\n",
    "        except KeyError:\n",
    "            info['death'] = None\n",
    "            \n",
    "    try:\n",
    "        page.get_wikidata()\n",
    "        info['end_date'] = page.data['wikidata']['end time (P582)']\n",
    "    except KeyError:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['end_date'] = poly_page.data['infobox']['date']\n",
    "        except KeyError:\n",
    "            info['end_date'] = None\n",
    "    \n",
    "    try:\n",
    "        page.get_wikidata()\n",
    "        info['location'] = page.data['wikidata']['location (P276)']\n",
    "    except KeyError:\n",
    "        try:\n",
    "            page.get_parse()\n",
    "            info['location'] = poly_page.data['infobox']['place']\n",
    "        except KeyError:\n",
    "            info['location'] = None\n",
    "    \n",
    "    \n",
    "    return Row(id=entity.id, title=entity.title, death=info['death'],\n",
    "               end_date=info['end_date'], location=info['location']) #, location=info['location'])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNICORN ON THE GOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying access to infobox\n",
    "poly_page = wptools.page('S11 (protest)')\n",
    "poly_page.get_parse()\n",
    "poly_page.get_wikidata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_page.data['infobox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_page.data['wikidata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2 = wikipedia.filter(\"title like '%World War II%'\") #World War II for military, civilian attack 1993 World Trade Center bombing\n",
    "ww2_df = sqlContext.createDataFrame(ww2.rdd.map(get_wiki_military_conflict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ww2_df.select('location').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_df.select('end_date').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_df.select('title').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_df.select('death').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_df.select('id').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
